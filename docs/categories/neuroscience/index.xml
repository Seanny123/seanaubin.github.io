<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Neuroscience on Voyageur Technologique</title>
    <link>https://seanaubin.github.io/categories/neuroscience/index.xml</link>
    <description>Recent content in Neuroscience on Voyageur Technologique</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <atom:link href="https://seanaubin.github.io/categories/neuroscience/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Thing Explainer: Spaun</title>
      <link>https://seanaubin.github.io/post/2017-04-16-Thing-Explainer-Spaun/</link>
      <pubDate>Sun, 16 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>https://seanaubin.github.io/post/2017-04-16-Thing-Explainer-Spaun/</guid>
      <description>

&lt;p&gt;&lt;em&gt;[epistemic status: poorly written, potentially missing the point and desperately in need of some figures]&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Today I found a lab’s blog that explained everything using only &lt;a href=&#34;https://xkcd.com/thing-explainer/&#34;&gt;the 1000 most common words in the English language in the manner popularised by XKCD&lt;/a&gt;. I thought I would try to do it for my lab’s philosophy/research and see how far I got. Also, because I frequently send my papers to my parents and I’m pretty sure they don’t understand anything.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;a href=&#34;http://science.sciencemag.org/content/338/6111/1202&#34;&gt;We built something like a brain using a computer!&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;why&#34;&gt;Why?&lt;/h2&gt;

&lt;p&gt;We wanted to make something that’s like-a-brain in a computer for many reasons.&lt;/p&gt;

&lt;p&gt;Because when you put a like-a-brain in a computer you can try to make it do different things. Like you can&amp;rsquo;t do to people who have real brains, because it would be mean or take to much time/money to do.&lt;/p&gt;

&lt;p&gt;Also, people have been trying to make very-good-at-thinking computers for a while and they did a pretty good job. We have computers who are very good at one thing-that-people-are-good-at. But we have no computers good at a lots of things-that-people-are-good-at. Also, we have no computers taking what they learned from one thing (like climbing stairs) and then using what they learned to many other things (like climbing a tree). We think to do this type of learning we should build like-a-brains for computers, since we’re trying to make computers be like humans who are using their real brains.&lt;/p&gt;

&lt;h2 id=&#34;is-it-a-good&#34;&gt;Is it a good?&lt;/h2&gt;

&lt;p&gt;It’s okay. A perfect like-a-brain would be exactly like a brain. Ours is kind of like a brain. It’s close in some ways and far in other ways.&lt;/p&gt;

&lt;p&gt;To check if your like-a-brain is good, you have to ask questions like:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Can it do the same things human brains do? Does it do the same things in the same human-like ways?&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Does it get the same things wrong that people do?&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;When we read from your like-a-brain in the same way we read from real brains in hospitals, do we read the same thing?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To see if it’s actually a good like-a-brain, your one like-a-brain should do a lot of different things. Because real brains do a lot of different things. But most like-a-brains only do one thing. This is bad. Our like-a-brain does a lot of different things. It can read and draw numbers. It can do simple number problems. It can solve some other problems usually used to check how good people are at thinking. It does these things the same way humans do them. So our like-a-brain is okay, but it’s not great because it can’t do that many things yet. We need to keep working to build a better like-a-brain.&lt;/p&gt;

&lt;p&gt;If we built a good like-a-brain, then we should also read stuff from it. We can read stuff from our like-a-brain, because we built it with something close to actual brain-parts. Actual brain-parts are tiny things joined to each other by lines they use for talking. They talk with these lines by sending pointed waves down the lines. We imitated actual brain-parts. We read from the brain-parts. What we read from like-a-brain brain-parts was close what we would read from real brain brain-parts!&lt;/p&gt;

&lt;p&gt;Those brain-parts also do other good things. They’re very good at working together even if some of the brain-parts die or get hurt. Also, they use very little food. Using very little food is important for computers that move around and are not plugged in, because they need to take their computer-food with them and carrying computer-food is hard.&lt;/p&gt;

&lt;p&gt;The best thing about using good like-brain-parts is that we can do things to our like-a-brain that we talked about before, like give get-not-sick-food or hurt. Then we can tell people who have the same brain-hurt what to do to get better, given what we saw the like-a-brain do.&lt;/p&gt;

&lt;h2 id=&#34;how&#34;&gt;How?&lt;/h2&gt;

&lt;p&gt;Now you know why we built a like-a-brain and why we think our like-a-brain is good. But how did we build the like-a-brain?&lt;/p&gt;

&lt;p&gt;Math. Lots of math. So much math. Years of math.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;If you liked this article and want to read more of my brain science posts, consider&lt;/em&gt; &lt;a href=&#34;https://uwaterloo.us15.list-manage.com/subscribe?u=d5612fe997cc72aac70c4ffe9&amp;amp;id=76226838bc&#34;&gt;&lt;em&gt;subscribing to my mailing list&lt;/em&gt;&lt;/a&gt;&lt;em&gt;.&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Deep Learning is almost the brain</title>
      <link>https://seanaubin.github.io/post/2017-01-09-Deep-Learning-is-almost-the-brain/</link>
      <pubDate>Mon, 09 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>https://seanaubin.github.io/post/2017-01-09-Deep-Learning-is-almost-the-brain/</guid>
      <description>

&lt;p&gt;&lt;em&gt;[epistemic status: an update on a&lt;/em&gt; &lt;a href=&#34;https://hackernoon.com/deep-learning-isnt-the-brain-e1d800ebb5a9#.qkv2fs6ah&#34;&gt;&lt;em&gt;previous position&lt;/em&gt;&lt;/a&gt; &lt;em&gt;as a result of discussion with people smarter than me]&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;People emailed me and I read their stuff! As I predicted, I was sometimes horribly wrong and often too simplistic.&lt;/p&gt;

&lt;p&gt;


  
    &lt;figure &gt;
  



  &lt;label for=&#34;&#34; class=&#34;margin-toggle&#34;&gt;⊕&lt;/label&gt;
  &lt;input type=&#34;checkbox&#34; id=&#34;&#34; class=&#34;margin-toggle&#34;&gt;
  &lt;span class=&#34;marginnote&#34;&gt;
  

  
  It currently has 800 views, which is 10 times more than my usual readership.
  
  
  

  &lt;/span&gt;


  
  &lt;img src=&#34;https://seanaubin.github.io/img/Deep-Learning-is-almost-the-brain_0.png&#34; &gt;
  



&lt;/figure&gt;




  &lt;/section&gt;

    
&lt;/p&gt;

&lt;p&gt;Below is an update/clarification on my positions.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;some-form-of-back-prop-could-be-biologically-plausible&#34;&gt;Some form of back-prop could be biologically plausible&lt;/h2&gt;

&lt;p&gt;There is &lt;a href=&#34;http://cogsci.stackexchange.com/q/16269/4397&#34;&gt;a ton of work&lt;/a&gt; on biologically plausible back-prop and it was really unfair of me to dismiss them all in one paragraph. That being said, I’m still sceptical of the various implementations, due to the fact they seem to exist in isolation of each other and I can’t find any of their code anywhere (except for “&lt;a href=&#34;https://arxiv.org/abs/1610.00161&#34;&gt;Deep learning with segregated dendrites&lt;/a&gt;”) so I can evaluate them.&lt;/p&gt;

&lt;h2 id=&#34;against-theoretical-isolation-a-manifesto&#34;&gt;Against theoretical isolation: a manifesto&lt;/h2&gt;

&lt;p&gt;My new, much better informed position on DL is that I wish it didn’t exist in such isolation. &lt;a href=&#34;https://medium.com/@seanaubin/understanding-the-brain-where-metaphors-limit-you-13d5d5fbdc57#.9fw8ikmog&#34;&gt;As I’ve written before&lt;/a&gt;, I think a promising line of research is integrating it with other approaches to cognitive modelling. If there was some way it was possible to move up and down the ladder of abstraction with DL, I would consider it a much better analogy for neural computation. Additionally, this would resolve the other weaker arguments (about DL not leveraging biological details, the limitations of current neuromorphic hardware and the lack of spike use in learning) from my previous post. Finally, this would also allow for the leveraging of the powerful developmental explanations allowed by DL by other cognitive modelling paradigms.&lt;/p&gt;

&lt;p&gt;


  
    &lt;figure &gt;
  



  &lt;label for=&#34;&#34; class=&#34;margin-toggle&#34;&gt;⊕&lt;/label&gt;
  &lt;input type=&#34;checkbox&#34; id=&#34;&#34; class=&#34;margin-toggle&#34;&gt;
  &lt;span class=&#34;marginnote&#34;&gt;
  

  
  The Cognitive Modelling [Ladder of Abstraction](http://worrydream.com/LadderOfAbstraction/). ([image source](https://docs.google.com/drawings/d/1mknlMUpnFzRXpb3L_dt-x8hxYK42b1pxlNfYstDH8uc/edit?usp=sharing))
  
  
  

  &lt;/span&gt;


  
  &lt;img src=&#34;https://seanaubin.github.io/img/Deep-Learning-is-almost-the-brain_0.png&#34; &gt;
  



&lt;/figure&gt;




  &lt;/section&gt;

    
&lt;/p&gt;

&lt;p&gt;I realise this is a purely philosophical position about what a good future for cognitive modelling looks like. I’m sure there are detractors who consider it unnecessary to connect to different approaches via biology. However I feel like a lack of synthesis can lead to overly complicated, unscalable models. For example, &lt;a href=&#34;http://compneuro.uwaterloo.ca/publications/crawford2015.html&#34;&gt;Eric Crawford’s model of knowledge representation&lt;/a&gt;, which demonstrated how using symbols and neurons allows for much more scalable model than a purely neural approach. Alternatively, consider &lt;a href=&#34;http://science.sciencemag.org/content/338/6111/1202&#34;&gt;Spaun&lt;/a&gt;. It is only able to perform many tasks and generate many predictions as a result of mixing different approaches which would only be possible with a much greater increase in complexity. However these are feelings, not facts.&lt;/p&gt;

&lt;p&gt;


  
    &lt;figure &gt;
  



  &lt;label for=&#34;&#34; class=&#34;margin-toggle&#34;&gt;⊕&lt;/label&gt;
  &lt;input type=&#34;checkbox&#34; id=&#34;&#34; class=&#34;margin-toggle&#34;&gt;
  &lt;span class=&#34;marginnote&#34;&gt;
  

  
  ([source](https://twitter.com/umruehren/status/816665138161799168))
  
  
  

  &lt;/span&gt;


  
  &lt;img src=&#34;https://seanaubin.github.io/img/Deep-Learning-is-almost-the-brain_1.png&#34; &gt;
  



&lt;/figure&gt;




  &lt;/section&gt;

    
&lt;/p&gt;

&lt;p&gt;The only way I can see my philosophy can be proved wrong is if cognitive modelling advances without consulting biology. In which case I will abandon my pretences and join everyone in the realm of pure mathematics. Until then I’ll keep trying to build my ladder.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;This philosophical approach isn’t easy and I’m sure others have attempted this. I’m just not clear on why the other attempts failed. Is it a &lt;a href=&#34;https://medium.com/@seanaubin/the-surprising-things-i-learned-from-grad-school-8a0efd458ae0#.94hfg15yq&#34;&gt;lack of reproducible computational experiments due to various problems&lt;/a&gt;? Or was it just too difficult until now? I look forward to working to find out.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;If you liked this article and want to read more of my brain science posts, consider&lt;/em&gt; &lt;a href=&#34;https://uwaterloo.us15.list-manage.com/subscribe?u=d5612fe997cc72aac70c4ffe9&amp;amp;id=76226838bc&#34;&gt;&lt;em&gt;subscribing to my mailing list&lt;/em&gt;&lt;/a&gt;&lt;em&gt;.&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Deep Learning isn’t the brain</title>
      <link>https://seanaubin.github.io/post/2016-11-17-Deep-Learning-isn-t-the-brain/</link>
      <pubDate>Thu, 17 Nov 2016 00:00:00 +0000</pubDate>
      
      <guid>https://seanaubin.github.io/post/2016-11-17-Deep-Learning-isn-t-the-brain/</guid>
      <description>

&lt;p&gt;&lt;em&gt;[epistemic status: I work in a lab dedicated to biologically plausible neural circuits, so I’m informed on the problem, but probably still biased. There’s probably going be a follow-up post to this once I get a bunch of rebuttals from people smarter than me.]&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Update:&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;As expected, I got a bunch of rebuttals and have&lt;/em&gt; &lt;a href=&#34;https://medium.com/@seanaubin/deep-learning-is-almost-the-brain-3aaecd924f3d&#34;&gt;&lt;em&gt;adjusted my position accordingly&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I keep seeing &lt;a href=&#34;http://biorxiv.org/content/biorxiv/early/2016/08/23/071076.full.pdf&#34;&gt;academic&lt;/a&gt; and &lt;a href=&#34;http://timdettmers.com/2015/07/27/brain-vs-deep-learning-singularity/&#34;&gt;non-academic articles&lt;/a&gt; comparing Deep Learning (DL) and the brain. This offends my sensibilities a bit, because although there are &lt;a href=&#34;http://compneuro.uwaterloo.ca/publications/sharma2016b.html&#34;&gt;many&lt;/a&gt; &lt;a href=&#34;http://www.pnas.org/content/111/23/8619.abstract&#34;&gt;results&lt;/a&gt; from DL that resembles certain areas of the brain, DL is not a good overall description of the brain. DL explicitly passes the buck on biological plausibility (like almost every other cognitive modelling approach) and implies that it’s “neurons” can be implemented biologically, it’s just that no one has bothered yet. I think the problem goes much deeper and that DL is missing a lot of the key features of the brain, which makes it a poor analogical target.&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&#34;the-brain-is-low-power&#34;&gt;The brain is low power&lt;/h4&gt;

&lt;p&gt;DL is power hungry. Alpha GO consumed the power of &lt;a href=&#34;https://www.tastehit.com/blog/google-deepmind-alphago-how-it-works/&#34;&gt;1202 CPUs and 176 GPUs&lt;/a&gt;, not to train, but &lt;em&gt;just to run&lt;/em&gt;. The &lt;a href=&#34;https://cloudplatform.googleblog.com/2016/05/Google-supercharges-machine-learning-tasks-with-custom-chip.html&#34;&gt;TenserFlow Processing Unit&lt;/a&gt; is an attempt to satiate this hunger, but it’s still not even close to the &lt;a href=&#34;http://cogsci.stackexchange.com/q/12750/4397&#34;&gt;brain’s power consumption of 20W&lt;/a&gt;. IBM’s TrueNorth chip is another example of trying to bring low-power computation, but &lt;a href=&#34;https://medium.com/@seanaubin/a-way-around-the-coming-performance-walls-neuromorphic-hardware-with-spiking-neurons-facd4291b201#.86xjo9oyf&#34;&gt;it’s capabilities are quite limited when compared to other Neuromorphic hardware&lt;/a&gt;. Specifically, True North only implements feed-forward networks and has no on-chip learning.&lt;/p&gt;

&lt;h4 id=&#34;the-brain-can-t-do-back-prop&#34;&gt;The brain can’t do back-prop&lt;/h4&gt;

&lt;p&gt;Back-propagation is the foundation of all DL. Although there is evidence that errors being propagated through multiple layers is happening in the brain, no one has come up with a method for back-propagation (back-prop)that doesn’t rely on information propagating backwards through unidirectional synapses. I personally think it’s only a matter of time before a biologically plausible method is discovered, but until then it is unwise to ignore the implementation details and the restrictions it might place on what can be learned.&lt;/p&gt;

&lt;h4 id=&#34;the-brain-uses-spikes-to-communicate&#34;&gt;The brain uses spikes to communicate&lt;/h4&gt;

&lt;p&gt;Although it is possible to &lt;a href=&#34;https://arxiv.org/abs/1510.08829&#34;&gt;convert DL networks into spiking neurons for use on neuromorphic hardware&lt;/a&gt;, these spikes are not leveraged for specific computational advantages. As far as I know (and I still have some reading to do), spiking computation has yet to be used anywhere for the learning in DL.&lt;/p&gt;

&lt;h4 id=&#34;neurotransmitters-aren-t-just-spike-transporters-and-neurons-aren-t-just-spike-machines&#34;&gt;Neurotransmitters aren’t just spike transporters and neurons aren’t just spike-machines&lt;/h4&gt;

&lt;p&gt;DL completely ignores the role of neurotransmitters. However, neurotransmitters have been shown to be computationally significant in adapting the receptive features networks on the fly, something which DL has really hard time doing.&lt;/p&gt;

&lt;h4 id=&#34;the-brain-is-noisy&#34;&gt;The brain is noisy&lt;/h4&gt;

&lt;p&gt;Given the choice between neuron redundancy and neuron performance, evolution chose to make the brain redundant. &lt;a href=&#34;http://icwww.epfl.ch/~gerstner/SPNM/node33.html&#34;&gt;Neurons are noisy&lt;/a&gt;, which isn’t surprising when you consider the warm, biologically variable environment they’re in. Although certain DL networks can cope with loss of their nodes, DL isn’t known for it’s robustness to noisy input or noisy training data.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;In conclusion, there are a lot of features of the brain that DL is omitting, thus using DL as an analogy for neural circuitry isn’t ideal. The alternative to comparing with DL is using a modelling paradigm that takes these challenges into account. At the time of writing, the only approach I know of is the Neural Engineering Framework (NEF) from the laboratory I belong to, but I’m sure as research marches forward other frameworks will emerge.&lt;/p&gt;

&lt;p&gt;“But Sean,” you cry with a gleam of mischief in your eye, “don’t most NEF models from the lab you belong to suffer from the same problems as DL? The models usually stop at LIF neurons, which aren’t realistic neurons at all! Why don’t you use more complex neuron models, i.e. things like &lt;a href=&#34;http://pub.ist.ac.at/Pubs/courses/AY1314/MolandCellNeuro_S14/files/Stuart%20et%20al_Dendrites_Chapter16.pdf&#34;&gt;dendritic computation&lt;/a&gt;, multi-compartmental neurons, &lt;a href=&#34;http://www.cell.com/trends/immunology/abstract/S1471-4906%2815%2900200-8&#34;&gt;glial cells&lt;/a&gt; and neurogenesis?”&lt;/p&gt;

&lt;p&gt;That’s a work in progress. There are 2 people (&lt;a href=&#34;http://compneuro.uwaterloo.ca/people/aaron-russell-voelker.html&#34;&gt;Aaron Voelker&lt;/a&gt; and &lt;a href=&#34;http://compneuro.uwaterloo.ca/people/peter-duggins.html&#34;&gt;Peter Duggins&lt;/a&gt;) of the 16-strong &lt;a href=&#34;http://compneuro.uwaterloo.ca/index.html&#34;&gt;Computation Neuroscience Research Group&lt;/a&gt; (CNRG) that I belong to who are working on the problem of more complex neuron models. &lt;a href=&#34;http://compneuro.uwaterloo.ca/people/eric-hunsberger.html&#34;&gt;Eric Hunsberger&lt;/a&gt; is working on biologically plausible back-prop and the moment he has a break-through you can be sure I’ll be shoving it in everyone’s face.&lt;/p&gt;

&lt;p&gt;As for neurogenesis, There’s no good computational model of what neurogenesis does and the CNRG lacks the resources to do that sort of basic research. Dendritic and glial cell computation has no one working on it, because we’re only 16 people. If that bothers you, maybe you want to join us?&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Modelling the Divide Between Conscious and Subconscious</title>
      <link>https://seanaubin.github.io/post/2016-03-01-Modelling-the-Divide-Between-Conscious-and-Subconscious/</link>
      <pubDate>Tue, 01 Mar 2016 00:00:00 +0000</pubDate>
      
      <guid>https://seanaubin.github.io/post/2016-03-01-Modelling-the-Divide-Between-Conscious-and-Subconscious/</guid>
      <description>&lt;p&gt;&lt;em&gt;[epistemic status: although I&amp;rsquo;ve become adept with the Neural Engineering Framework and the Semantic Pointer Architecture, I&amp;rsquo;m not super comfortable in the philosophical depths of Cognitive Science. This article is not a professional presenting a final answer, but an amateur formulating their current view.]&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The divide between what is conscious and what is subconscious is a fascinating question often because it acts as a proxy for more urgent personal questions, such as:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Who am I?&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;How much can I change?&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;What do I have control over?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In this article, I won’t attempt to answer these questions directly (given they’re freaking hard questions), instead I’d like to approach this question in the only way I know how: by building a model based off of empirical, experimental and philosophical evidence. By “building” I mean just putting together two models that have already been built and by “evidence” I mean using the evidence in the papers that I took the models from, without explicitly stating it here.&lt;/p&gt;

&lt;p&gt;To understand the subconscious, we first have to understand what is consciousness. To model consciousness we need to select our tools. The modelling tool that I find the most power in (surprising no one given it comes from my lab) is the Semantic Pointer Architecture (SPA).&lt;/p&gt;

&lt;p&gt;The SPA is based on the idea that computation in the brain is done by manipulating vectors called Semantic Pointers (SPs) by binding and compressing them using neurons. This idea is kind of sophisticated, but the easiest way to understand it is seeing Spaun, which was built using the SPA, in action.&lt;/p&gt;

&lt;iframe frameborder=&#34;0&#34; height=&#34;480&#34; scrolling=&#34;no&#34; src=&#34;https://www.youtube.com/embed/mP7DX6x9PX8?feature=oembed&#34; width=&#34;640&#34;&gt;&lt;/iframe&gt;

&lt;p&gt;In the above video, Spaun takes the high dimensional input of the many pixels on a screen, transforms it into the concept of ‘4’ and ‘3’ to perform the task by going up the visual hierarchy, and then writes it the number back out by going down the motor hierarchy.&lt;/p&gt;

&lt;p&gt;This shows how manipulating vectors is a model of brain processes. It also shows how concepts are tied to the sensory information received from the outside world.&lt;/p&gt;

&lt;p&gt;How can this be applied to consciousness? In &lt;a href=&#34;http://cogsci.uwaterloo.ca/Articles/thagard.two-theories.consc&amp;amp;cog.2014.pdf&#34;&gt;this model by Paul Thagard and Terry Stewart&lt;/a&gt;, consciousness is modelled as competition between SPs.&lt;/p&gt;

&lt;p&gt;


  
    &lt;figure &gt;
  



  &lt;label for=&#34;&#34; class=&#34;margin-toggle&#34;&gt;⊕&lt;/label&gt;
  &lt;input type=&#34;checkbox&#34; id=&#34;&#34; class=&#34;margin-toggle&#34;&gt;
  &lt;span class=&#34;marginnote&#34;&gt;
  

  
  Taken from Thagard and Stewart 2014
  
  
  

  &lt;/span&gt;


  
  &lt;img src=&#34;https://seanaubin.github.io/img/Modelling-the-Divide-Between-Conscious-and-Subconscious_0.png&#34; &gt;
  



&lt;/figure&gt;




  &lt;/section&gt;

    
&lt;/p&gt;

&lt;p&gt;Basically, it’s saying you get representations in your brain from your motor system, your sensory inputs, your emotions and your verbal productions. What you become conscious of is a result of one of those representations winning/losing the fight for your attention.&lt;/p&gt;

&lt;p&gt;That’s consciousness, so everything below that must be subconscious. Most of those inputs are pretty easily explainable, except emotion. What is emotion? There’s a model for that too using SPs in &lt;a href=&#34;http://cogsci.uwaterloo.ca/Articles/thagard-schroeder.emotions-pointers.2013.pdf&#34;&gt;this book chapter by Paul Thagard and Tobias Schröder&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;


  
    &lt;figure &gt;
  



  &lt;label for=&#34;&#34; class=&#34;margin-toggle&#34;&gt;⊕&lt;/label&gt;
  &lt;input type=&#34;checkbox&#34; id=&#34;&#34; class=&#34;margin-toggle&#34;&gt;
  &lt;span class=&#34;marginnote&#34;&gt;
  

  
  Taken from Thagard and Schröder 2013
  
  
  

  &lt;/span&gt;


  
  &lt;img src=&#34;https://seanaubin.github.io/img/Modelling-the-Divide-Between-Conscious-and-Subconscious_1.png&#34; &gt;
  



&lt;/figure&gt;




  &lt;/section&gt;

    
&lt;/p&gt;

&lt;p&gt;It argues that emotion is a result of a combination of neural representations of your physiology (am I hot/cold?), your situation (can I turn up the thermostat?), and your appraisal (what do I think about all of this?).&lt;/p&gt;

&lt;p&gt;Putting these two together, you get something that looks like this.&lt;/p&gt;

&lt;p&gt;


  
    &lt;figure &gt;
  



  &lt;label for=&#34;&#34; class=&#34;margin-toggle&#34;&gt;⊕&lt;/label&gt;
  &lt;input type=&#34;checkbox&#34; id=&#34;&#34; class=&#34;margin-toggle&#34;&gt;
  &lt;span class=&#34;marginnote&#34;&gt;
  

  
  You can tell I made this figure because of it came from Google Draw
  
  
  

  &lt;/span&gt;


  
  &lt;img src=&#34;https://seanaubin.github.io/img/Modelling-the-Divide-Between-Conscious-and-Subconscious_2.png&#34; &gt;
  



&lt;/figure&gt;




  &lt;/section&gt;

    
&lt;/p&gt;

&lt;p&gt;This is all very abstract and hard to follow, so let’s use a more practical example in the form of someone who has anxiety. Because often the best way to understand a system is to look at it while it’s malfunctioning and (more importantly) coming up with ways to fix it.&lt;/p&gt;

&lt;p&gt;Anxiety has multiple causes and thus multiple solutions. Sometimes it can be a product of a situation, in which case the situation can be fixed or avoided using action. Other times the cause is less concrete, arriving in the form of a maladaptive thought pattern, in which case the appraisal part of this system needs to be remedied, possibly using &lt;a href=&#34;https://medium.com/@seanaubin/debugging-your-thoughts-with-mind-maps-94aeecd2821d#.x7tekal3x&#34;&gt;Cognitive Behaviour Therapy&lt;/a&gt;. Finally, there’s the possibility that physiologically the person consistently finds they are constantly in “flight or fight” mode without cause, which could be remedied with medication. Obviously every case of anxiety disorder is unique and which options are pursued are up to the individual.&lt;/p&gt;

&lt;p&gt;This is one view of consciousness, which provides some useful explanatory mechanisms, but I&amp;rsquo;m certainly not claiming it’s the right one. I&amp;rsquo;m currently trying to gain more subtlety in my perspective on this matter by reading different perspectives, such as &lt;a href=&#34;https://www.youtube.com/watch?v=2o2xBOQeB7Q&#34;&gt;Joscha Bach&lt;/a&gt;’s MicroPsi framework. I’ll write another post (or more likely post a question/answer on &lt;a href=&#34;https://cogsci.stackexchange.com/tour&#34;&gt;CogSci.SE&lt;/a&gt;) once I&amp;rsquo;ve put my thoughts together.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>