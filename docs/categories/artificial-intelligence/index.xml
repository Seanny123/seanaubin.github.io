<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Artificial Intelligence on Voyageur Technologique</title>
    <link>https://seanaubin.github.io/categories/artificial-intelligence/index.xml</link>
    <description>Recent content in Artificial Intelligence on Voyageur Technologique</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <atom:link href="https://seanaubin.github.io/categories/artificial-intelligence/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Thing Explainer: Spaun</title>
      <link>https://seanaubin.github.io/post/2017-04-16-Thing-Explainer-Spaun/</link>
      <pubDate>Sun, 16 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>https://seanaubin.github.io/post/2017-04-16-Thing-Explainer-Spaun/</guid>
      <description>

&lt;p&gt;&lt;em&gt;[epistemic status: poorly written, potentially missing the point and desperately in need of some figures]&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Today I found a lab’s blog that explained everything using only &lt;a href=&#34;https://xkcd.com/thing-explainer/&#34;&gt;the 1000 most common words in the English language in the manner popularised by XKCD&lt;/a&gt;. I thought I would try to do it for my lab’s philosophy/research and see how far I got. Also, because I frequently send my papers to my parents and I’m pretty sure they don’t understand anything.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;a href=&#34;http://science.sciencemag.org/content/338/6111/1202&#34;&gt;We built something like a brain using a computer!&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;why&#34;&gt;Why?&lt;/h2&gt;

&lt;p&gt;We wanted to make something that’s like-a-brain in a computer for many reasons.&lt;/p&gt;

&lt;p&gt;Because when you put a like-a-brain in a computer you can try to make it do different things. Like you can&amp;rsquo;t do to people who have real brains, because it would be mean or take to much time/money to do.&lt;/p&gt;

&lt;p&gt;Also, people have been trying to make very-good-at-thinking computers for a while and they did a pretty good job. We have computers who are very good at one thing-that-people-are-good-at. But we have no computers good at a lots of things-that-people-are-good-at. Also, we have no computers taking what they learned from one thing (like climbing stairs) and then using what they learned to many other things (like climbing a tree). We think to do this type of learning we should build like-a-brains for computers, since we’re trying to make computers be like humans who are using their real brains.&lt;/p&gt;

&lt;h2 id=&#34;is-it-a-good&#34;&gt;Is it a good?&lt;/h2&gt;

&lt;p&gt;It’s okay. A perfect like-a-brain would be exactly like a brain. Ours is kind of like a brain. It’s close in some ways and far in other ways.&lt;/p&gt;

&lt;p&gt;To check if your like-a-brain is good, you have to ask questions like:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Can it do the same things human brains do? Does it do the same things in the same human-like ways?&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Does it get the same things wrong that people do?&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;When we read from your like-a-brain in the same way we read from real brains in hospitals, do we read the same thing?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To see if it’s actually a good like-a-brain, your one like-a-brain should do a lot of different things. Because real brains do a lot of different things. But most like-a-brains only do one thing. This is bad. Our like-a-brain does a lot of different things. It can read and draw numbers. It can do simple number problems. It can solve some other problems usually used to check how good people are at thinking. It does these things the same way humans do them. So our like-a-brain is okay, but it’s not great because it can’t do that many things yet. We need to keep working to build a better like-a-brain.&lt;/p&gt;

&lt;p&gt;If we built a good like-a-brain, then we should also read stuff from it. We can read stuff from our like-a-brain, because we built it with something close to actual brain-parts. Actual brain-parts are tiny things joined to each other by lines they use for talking. They talk with these lines by sending pointed waves down the lines. We imitated actual brain-parts. We read from the brain-parts. What we read from like-a-brain brain-parts was close what we would read from real brain brain-parts!&lt;/p&gt;

&lt;p&gt;Those brain-parts also do other good things. They’re very good at working together even if some of the brain-parts die or get hurt. Also, they use very little food. Using very little food is important for computers that move around and are not plugged in, because they need to take their computer-food with them and carrying computer-food is hard.&lt;/p&gt;

&lt;p&gt;The best thing about using good like-brain-parts is that we can do things to our like-a-brain that we talked about before, like give get-not-sick-food or hurt. Then we can tell people who have the same brain-hurt what to do to get better, given what we saw the like-a-brain do.&lt;/p&gt;

&lt;h2 id=&#34;how&#34;&gt;How?&lt;/h2&gt;

&lt;p&gt;Now you know why we built a like-a-brain and why we think our like-a-brain is good. But how did we build the like-a-brain?&lt;/p&gt;

&lt;p&gt;Math. Lots of math. So much math. Years of math.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;If you liked this article and want to read more of my brain science posts, consider&lt;/em&gt; &lt;a href=&#34;https://uwaterloo.us15.list-manage.com/subscribe?u=d5612fe997cc72aac70c4ffe9&amp;amp;id=76226838bc&#34;&gt;&lt;em&gt;subscribing to my mailing list&lt;/em&gt;&lt;/a&gt;&lt;em&gt;.&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Deep Learning is almost the brain</title>
      <link>https://seanaubin.github.io/post/2017-01-09-Deep-Learning-is-almost-the-brain/</link>
      <pubDate>Mon, 09 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>https://seanaubin.github.io/post/2017-01-09-Deep-Learning-is-almost-the-brain/</guid>
      <description>

&lt;p&gt;&lt;em&gt;[epistemic status: an update on a&lt;/em&gt; &lt;a href=&#34;https://hackernoon.com/deep-learning-isnt-the-brain-e1d800ebb5a9#.qkv2fs6ah&#34;&gt;&lt;em&gt;previous position&lt;/em&gt;&lt;/a&gt; &lt;em&gt;as a result of discussion with people smarter than me]&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;People emailed me and I read their stuff! As I predicted, I was sometimes horribly wrong and often too simplistic.&lt;/p&gt;

&lt;p&gt;


  
    &lt;figure &gt;
  



  &lt;label for=&#34;&#34; class=&#34;margin-toggle&#34;&gt;⊕&lt;/label&gt;
  &lt;input type=&#34;checkbox&#34; id=&#34;&#34; class=&#34;margin-toggle&#34;&gt;
  &lt;span class=&#34;marginnote&#34;&gt;
  

  
  It currently has 800 views, which is 10 times more than my usual readership.
  
  
  

  &lt;/span&gt;


  
  &lt;img src=&#34;https://seanaubin.github.io/img/Deep-Learning-is-almost-the-brain_0.png&#34; &gt;
  



&lt;/figure&gt;




  &lt;/section&gt;

    
&lt;/p&gt;

&lt;p&gt;Below is an update/clarification on my positions.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;some-form-of-back-prop-could-be-biologically-plausible&#34;&gt;Some form of back-prop could be biologically plausible&lt;/h2&gt;

&lt;p&gt;There is &lt;a href=&#34;http://cogsci.stackexchange.com/q/16269/4397&#34;&gt;a ton of work&lt;/a&gt; on biologically plausible back-prop and it was really unfair of me to dismiss them all in one paragraph. That being said, I’m still sceptical of the various implementations, due to the fact they seem to exist in isolation of each other and I can’t find any of their code anywhere (except for “&lt;a href=&#34;https://arxiv.org/abs/1610.00161&#34;&gt;Deep learning with segregated dendrites&lt;/a&gt;”) so I can evaluate them.&lt;/p&gt;

&lt;h2 id=&#34;against-theoretical-isolation-a-manifesto&#34;&gt;Against theoretical isolation: a manifesto&lt;/h2&gt;

&lt;p&gt;My new, much better informed position on DL is that I wish it didn’t exist in such isolation. &lt;a href=&#34;https://medium.com/@seanaubin/understanding-the-brain-where-metaphors-limit-you-13d5d5fbdc57#.9fw8ikmog&#34;&gt;As I’ve written before&lt;/a&gt;, I think a promising line of research is integrating it with other approaches to cognitive modelling. If there was some way it was possible to move up and down the ladder of abstraction with DL, I would consider it a much better analogy for neural computation. Additionally, this would resolve the other weaker arguments (about DL not leveraging biological details, the limitations of current neuromorphic hardware and the lack of spike use in learning) from my previous post. Finally, this would also allow for the leveraging of the powerful developmental explanations allowed by DL by other cognitive modelling paradigms.&lt;/p&gt;

&lt;p&gt;


  
    &lt;figure &gt;
  



  &lt;label for=&#34;&#34; class=&#34;margin-toggle&#34;&gt;⊕&lt;/label&gt;
  &lt;input type=&#34;checkbox&#34; id=&#34;&#34; class=&#34;margin-toggle&#34;&gt;
  &lt;span class=&#34;marginnote&#34;&gt;
  

  
  The Cognitive Modelling [Ladder of Abstraction](http://worrydream.com/LadderOfAbstraction/). ([image source](https://docs.google.com/drawings/d/1mknlMUpnFzRXpb3L_dt-x8hxYK42b1pxlNfYstDH8uc/edit?usp=sharing))
  
  
  

  &lt;/span&gt;


  
  &lt;img src=&#34;https://seanaubin.github.io/img/Deep-Learning-is-almost-the-brain_0.png&#34; &gt;
  



&lt;/figure&gt;




  &lt;/section&gt;

    
&lt;/p&gt;

&lt;p&gt;I realise this is a purely philosophical position about what a good future for cognitive modelling looks like. I’m sure there are detractors who consider it unnecessary to connect to different approaches via biology. However I feel like a lack of synthesis can lead to overly complicated, unscalable models. For example, &lt;a href=&#34;http://compneuro.uwaterloo.ca/publications/crawford2015.html&#34;&gt;Eric Crawford’s model of knowledge representation&lt;/a&gt;, which demonstrated how using symbols and neurons allows for much more scalable model than a purely neural approach. Alternatively, consider &lt;a href=&#34;http://science.sciencemag.org/content/338/6111/1202&#34;&gt;Spaun&lt;/a&gt;. It is only able to perform many tasks and generate many predictions as a result of mixing different approaches which would only be possible with a much greater increase in complexity. However these are feelings, not facts.&lt;/p&gt;

&lt;p&gt;


  
    &lt;figure &gt;
  



  &lt;label for=&#34;&#34; class=&#34;margin-toggle&#34;&gt;⊕&lt;/label&gt;
  &lt;input type=&#34;checkbox&#34; id=&#34;&#34; class=&#34;margin-toggle&#34;&gt;
  &lt;span class=&#34;marginnote&#34;&gt;
  

  
  ([source](https://twitter.com/umruehren/status/816665138161799168))
  
  
  

  &lt;/span&gt;


  
  &lt;img src=&#34;https://seanaubin.github.io/img/Deep-Learning-is-almost-the-brain_1.png&#34; &gt;
  



&lt;/figure&gt;




  &lt;/section&gt;

    
&lt;/p&gt;

&lt;p&gt;The only way I can see my philosophy can be proved wrong is if cognitive modelling advances without consulting biology. In which case I will abandon my pretences and join everyone in the realm of pure mathematics. Until then I’ll keep trying to build my ladder.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;This philosophical approach isn’t easy and I’m sure others have attempted this. I’m just not clear on why the other attempts failed. Is it a &lt;a href=&#34;https://medium.com/@seanaubin/the-surprising-things-i-learned-from-grad-school-8a0efd458ae0#.94hfg15yq&#34;&gt;lack of reproducible computational experiments due to various problems&lt;/a&gt;? Or was it just too difficult until now? I look forward to working to find out.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;If you liked this article and want to read more of my brain science posts, consider&lt;/em&gt; &lt;a href=&#34;https://uwaterloo.us15.list-manage.com/subscribe?u=d5612fe997cc72aac70c4ffe9&amp;amp;id=76226838bc&#34;&gt;&lt;em&gt;subscribing to my mailing list&lt;/em&gt;&lt;/a&gt;&lt;em&gt;.&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Deep Learning isn’t the brain</title>
      <link>https://seanaubin.github.io/post/2016-11-17-Deep-Learning-isn-t-the-brain/</link>
      <pubDate>Thu, 17 Nov 2016 00:00:00 +0000</pubDate>
      
      <guid>https://seanaubin.github.io/post/2016-11-17-Deep-Learning-isn-t-the-brain/</guid>
      <description>

&lt;p&gt;&lt;em&gt;[epistemic status: I work in a lab dedicated to biologically plausible neural circuits, so I’m informed on the problem, but probably still biased. There’s probably going be a follow-up post to this once I get a bunch of rebuttals from people smarter than me.]&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Update:&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;As expected, I got a bunch of rebuttals and have&lt;/em&gt; &lt;a href=&#34;https://medium.com/@seanaubin/deep-learning-is-almost-the-brain-3aaecd924f3d&#34;&gt;&lt;em&gt;adjusted my position accordingly&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I keep seeing &lt;a href=&#34;http://biorxiv.org/content/biorxiv/early/2016/08/23/071076.full.pdf&#34;&gt;academic&lt;/a&gt; and &lt;a href=&#34;http://timdettmers.com/2015/07/27/brain-vs-deep-learning-singularity/&#34;&gt;non-academic articles&lt;/a&gt; comparing Deep Learning (DL) and the brain. This offends my sensibilities a bit, because although there are &lt;a href=&#34;http://compneuro.uwaterloo.ca/publications/sharma2016b.html&#34;&gt;many&lt;/a&gt; &lt;a href=&#34;http://www.pnas.org/content/111/23/8619.abstract&#34;&gt;results&lt;/a&gt; from DL that resembles certain areas of the brain, DL is not a good overall description of the brain. DL explicitly passes the buck on biological plausibility (like almost every other cognitive modelling approach) and implies that it’s “neurons” can be implemented biologically, it’s just that no one has bothered yet. I think the problem goes much deeper and that DL is missing a lot of the key features of the brain, which makes it a poor analogical target.&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&#34;the-brain-is-low-power&#34;&gt;The brain is low power&lt;/h4&gt;

&lt;p&gt;DL is power hungry. Alpha GO consumed the power of &lt;a href=&#34;https://www.tastehit.com/blog/google-deepmind-alphago-how-it-works/&#34;&gt;1202 CPUs and 176 GPUs&lt;/a&gt;, not to train, but &lt;em&gt;just to run&lt;/em&gt;. The &lt;a href=&#34;https://cloudplatform.googleblog.com/2016/05/Google-supercharges-machine-learning-tasks-with-custom-chip.html&#34;&gt;TenserFlow Processing Unit&lt;/a&gt; is an attempt to satiate this hunger, but it’s still not even close to the &lt;a href=&#34;http://cogsci.stackexchange.com/q/12750/4397&#34;&gt;brain’s power consumption of 20W&lt;/a&gt;. IBM’s TrueNorth chip is another example of trying to bring low-power computation, but &lt;a href=&#34;https://medium.com/@seanaubin/a-way-around-the-coming-performance-walls-neuromorphic-hardware-with-spiking-neurons-facd4291b201#.86xjo9oyf&#34;&gt;it’s capabilities are quite limited when compared to other Neuromorphic hardware&lt;/a&gt;. Specifically, True North only implements feed-forward networks and has no on-chip learning.&lt;/p&gt;

&lt;h4 id=&#34;the-brain-can-t-do-back-prop&#34;&gt;The brain can’t do back-prop&lt;/h4&gt;

&lt;p&gt;Back-propagation is the foundation of all DL. Although there is evidence that errors being propagated through multiple layers is happening in the brain, no one has come up with a method for back-propagation (back-prop)that doesn’t rely on information propagating backwards through unidirectional synapses. I personally think it’s only a matter of time before a biologically plausible method is discovered, but until then it is unwise to ignore the implementation details and the restrictions it might place on what can be learned.&lt;/p&gt;

&lt;h4 id=&#34;the-brain-uses-spikes-to-communicate&#34;&gt;The brain uses spikes to communicate&lt;/h4&gt;

&lt;p&gt;Although it is possible to &lt;a href=&#34;https://arxiv.org/abs/1510.08829&#34;&gt;convert DL networks into spiking neurons for use on neuromorphic hardware&lt;/a&gt;, these spikes are not leveraged for specific computational advantages. As far as I know (and I still have some reading to do), spiking computation has yet to be used anywhere for the learning in DL.&lt;/p&gt;

&lt;h4 id=&#34;neurotransmitters-aren-t-just-spike-transporters-and-neurons-aren-t-just-spike-machines&#34;&gt;Neurotransmitters aren’t just spike transporters and neurons aren’t just spike-machines&lt;/h4&gt;

&lt;p&gt;DL completely ignores the role of neurotransmitters. However, neurotransmitters have been shown to be computationally significant in adapting the receptive features networks on the fly, something which DL has really hard time doing.&lt;/p&gt;

&lt;h4 id=&#34;the-brain-is-noisy&#34;&gt;The brain is noisy&lt;/h4&gt;

&lt;p&gt;Given the choice between neuron redundancy and neuron performance, evolution chose to make the brain redundant. &lt;a href=&#34;http://icwww.epfl.ch/~gerstner/SPNM/node33.html&#34;&gt;Neurons are noisy&lt;/a&gt;, which isn’t surprising when you consider the warm, biologically variable environment they’re in. Although certain DL networks can cope with loss of their nodes, DL isn’t known for it’s robustness to noisy input or noisy training data.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;In conclusion, there are a lot of features of the brain that DL is omitting, thus using DL as an analogy for neural circuitry isn’t ideal. The alternative to comparing with DL is using a modelling paradigm that takes these challenges into account. At the time of writing, the only approach I know of is the Neural Engineering Framework (NEF) from the laboratory I belong to, but I’m sure as research marches forward other frameworks will emerge.&lt;/p&gt;

&lt;p&gt;“But Sean,” you cry with a gleam of mischief in your eye, “don’t most NEF models from the lab you belong to suffer from the same problems as DL? The models usually stop at LIF neurons, which aren’t realistic neurons at all! Why don’t you use more complex neuron models, i.e. things like &lt;a href=&#34;http://pub.ist.ac.at/Pubs/courses/AY1314/MolandCellNeuro_S14/files/Stuart%20et%20al_Dendrites_Chapter16.pdf&#34;&gt;dendritic computation&lt;/a&gt;, multi-compartmental neurons, &lt;a href=&#34;http://www.cell.com/trends/immunology/abstract/S1471-4906%2815%2900200-8&#34;&gt;glial cells&lt;/a&gt; and neurogenesis?”&lt;/p&gt;

&lt;p&gt;That’s a work in progress. There are 2 people (&lt;a href=&#34;http://compneuro.uwaterloo.ca/people/aaron-russell-voelker.html&#34;&gt;Aaron Voelker&lt;/a&gt; and &lt;a href=&#34;http://compneuro.uwaterloo.ca/people/peter-duggins.html&#34;&gt;Peter Duggins&lt;/a&gt;) of the 16-strong &lt;a href=&#34;http://compneuro.uwaterloo.ca/index.html&#34;&gt;Computation Neuroscience Research Group&lt;/a&gt; (CNRG) that I belong to who are working on the problem of more complex neuron models. &lt;a href=&#34;http://compneuro.uwaterloo.ca/people/eric-hunsberger.html&#34;&gt;Eric Hunsberger&lt;/a&gt; is working on biologically plausible back-prop and the moment he has a break-through you can be sure I’ll be shoving it in everyone’s face.&lt;/p&gt;

&lt;p&gt;As for neurogenesis, There’s no good computational model of what neurogenesis does and the CNRG lacks the resources to do that sort of basic research. Dendritic and glial cell computation has no one working on it, because we’re only 16 people. If that bothers you, maybe you want to join us?&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Way Around the Coming Performance Walls: Neuromorphic Hardware with Spiking Neurons</title>
      <link>https://seanaubin.github.io/post/2015-09-30-A-Way-Around-the-Coming-Performance-Walls-Neuromorphic-Hardware-with-Spiking-Neurons/</link>
      <pubDate>Wed, 30 Sep 2015 00:00:00 +0000</pubDate>
      
      <guid>https://seanaubin.github.io/post/2015-09-30-A-Way-Around-the-Coming-Performance-Walls-Neuromorphic-Hardware-with-Spiking-Neurons/</guid>
      <description>

&lt;p&gt;&lt;em&gt;[epistemic status: I work in a lab focused on computations with neurons, so I&amp;rsquo;m probably a little biased.]&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Back in the day, when writing computing programs, you could always take comfort that even if your code had (non-exponential) bad performance Moore’s law would eventually save you from yourself.&lt;/p&gt;

&lt;p&gt;This isn&amp;rsquo;t very true anymore. There are two coming performance walls coming to hit us. Worse still, there’s another wall one we’re already suffering from!&lt;/p&gt;

&lt;p&gt;In this article, I’ll describe what these walls are, why they are such a problem and how neuromorphic hardware, especially those that work with spiking neurons, can offer a partial solution. I’ll also give a quick overview of the neuromorphic hardware my lab is developing as best as I understand it.&lt;/p&gt;

&lt;h2 id=&#34;moore-s-law-amdahl-s-law-and-the-energy-efficiency-wall&#34;&gt;Moore’s Law, Amdahl’s Law and the Energy Efficiency Wall&lt;/h2&gt;

&lt;p&gt;Moore’s law is the idea that the transistor density of a chip doubles every two years, which means faster computers. However, what’s this has translated into for the last few years is pretty massively parallel architectures with a bunch of cores, because improving single core performance has hit a bit of a ceiling. So instead of super fast single processors, our processors have ridiculous amounts of sub-processors and we’re now looking to co-processors, like the GPU, for even further acceleration.&lt;/p&gt;

&lt;p&gt;This means that now, more than ever, any code that you’re running serially instead of in parallel is going to be the bottle-neck. This is Amdahl’s law and it’s scary, because writing and compiling parallel code is hard.&lt;/p&gt;

&lt;p&gt;But maybe we’ll get really good at writing parallel code? Unfortunately, we can’t even rely on Moore’s law to keep up. Moore’s law is ending for a wide variety of reasons including &lt;a href=&#34;http://www.economist.com/blogs/economist-explains/2015/04/economist-explains-17&#34;&gt;economic&lt;/a&gt; and &lt;a href=&#34;http://arstechnica.com/science/2014/08/are-processors-pushing-up-against-the-limits-of-physics/&#34;&gt;physical&lt;/a&gt;, but worse still if we keep building processors as we traditionally do (this is called the Von Neumann architecture) we’re going to keep consuming more power.&lt;/p&gt;

&lt;p&gt;When I say the aforementioned traditional processors and their parallel co-processing brethren are power hungry, I&amp;rsquo;m talking specifically about how much power each calculation takes. It doesn&amp;rsquo;t look like it’s going to get any better any time soon. As you can see in the figure below, despite the fabrication process becoming more and more advanced over time (progressing from 130nm to ever smaller resolutions), the energy efficiency of their calculations isn&amp;rsquo;t increasing proportionately and we’re reaching an upper bound. This is called the Energy Efficiency Wall.&lt;/p&gt;

&lt;p&gt;


  
    &lt;figure &gt;
  



  &lt;label for=&#34;&#34; class=&#34;margin-toggle&#34;&gt;⊕&lt;/label&gt;
  &lt;input type=&#34;checkbox&#34; id=&#34;&#34; class=&#34;margin-toggle&#34;&gt;
  &lt;span class=&#34;marginnote&#34;&gt;
  

  
  Taken from [Finding a roadmap to achieve large scale neuromorphic systems](http://users.ece.gatech.edu/~phasler/Roadmap_paper_Hasler_2013.pdf). Thanks for the link Terry!
  
  
  

  &lt;/span&gt;


  
  &lt;img src=&#34;https://seanaubin.github.io/img/A-Way-Around-the-Coming-Performance-Walls-Neuromorphic-Hardware-with-Spiking-Neurons_0.png&#34; &gt;
  



&lt;/figure&gt;




  &lt;/section&gt;

    
&lt;/p&gt;

&lt;h2 id=&#34;neuromorphic-hardware&#34;&gt;Neuromorphic Hardware&lt;/h2&gt;

&lt;p&gt;One of the hopeful new technologies that might lead us around the performance walls is neuromorphic hardware. Neuromorphic hardware is currently an exploding field, which is cool, but also super confusing. Basically, the idea is to make something like a neuron in silicon, because neurons are cool. I&amp;rsquo;m not being facetious here, the applicability of modern neuromorphic hardware is super variable. Ranging from TrueNorth, which can used trained (back-prop or RBM) feed-forward artificial neural networks, to Standfords Neurogrid, which used analogue neurons for biologically focused experiments. And then there’s &lt;a href=&#34;http://higherintelligencebook.com/&#34;&gt;this guy&lt;/a&gt;, who seems to at least have the marketing angle mastered?&lt;/p&gt;

&lt;p&gt;One of the consistent optimizations that are happening across all these architectures, is that the memory is being placed closer to the processing unit. Like really close. Like chip-design level beside each other close. This saves a lot of power and a lot of time. So problem solved right? Except, now you have to be able to program these things. There’s a lot of interesting ideas around this. &lt;a href=&#34;http://www.infoq.com/presentations/power-144-chip&#34;&gt;Some people think you’re just going to need to use the same old programming languages, but just use them more carefully.&lt;/a&gt; My lab thinks that a whole new approach is necessary using spiking neural networks that we design with Nengo.&lt;/p&gt;

&lt;h2 id=&#34;spiking-neurons-and-nengo&#34;&gt;Spiking Neurons and Nengo&lt;/h2&gt;

&lt;p&gt;Nengo lets you compute in spiking neurons. Neuron groups approximate functions with non-linear encoders and linear decoders. Basically, thanks to neuroscience my lab’s figured out a way to go from the outside world to the distributed spiking world of neurons.&lt;/p&gt;

&lt;p&gt;Encoders mean the non-linear weighted input into a neuron. This means neurons have a preferred direction that means they fire more when something that they are “tuned” for comes in. Notice in the GIF below that only certain neurons fire for certain directions, as indicated by the arrow direction in their grey circle.&lt;/p&gt;

&lt;p&gt;


  
    &lt;figure &gt;
  



  &lt;label for=&#34;&#34; class=&#34;margin-toggle&#34;&gt;⊕&lt;/label&gt;
  &lt;input type=&#34;checkbox&#34; id=&#34;&#34; class=&#34;margin-toggle&#34;&gt;
  &lt;span class=&#34;marginnote&#34;&gt;
  

  
  All GIFs courtesy of Xuan Choo.
  
  
  

  &lt;/span&gt;


  
  &lt;img src=&#34;https://seanaubin.github.io/img/A-Way-Around-the-Coming-Performance-Walls-Neuromorphic-Hardware-with-Spiking-Neurons_1.gif&#34; &gt;
  



&lt;/figure&gt;




  &lt;/section&gt;

    
&lt;/p&gt;

&lt;p&gt;To get back out of the distributed neuron world, all we need is decoders. Decoders are linear weights on the output of a neuron that get summed together to either recreate the original signal (the top circle) or an arbitrary non-linear function (the bottom figure-eight).&lt;/p&gt;

&lt;p&gt;


  
    &lt;figure &gt;
  



  &lt;label for=&#34;&#34; class=&#34;margin-toggle&#34;&gt;⊕&lt;/label&gt;
  &lt;input type=&#34;checkbox&#34; id=&#34;&#34; class=&#34;margin-toggle&#34;&gt;
  &lt;span class=&#34;marginnote&#34;&gt;
  

  
  
  
  
  

  &lt;/span&gt;


  
  &lt;img src=&#34;https://seanaubin.github.io/img/A-Way-Around-the-Coming-Performance-Walls-Neuromorphic-Hardware-with-Spiking-Neurons_2.gif&#34; &gt;
  



&lt;/figure&gt;




  &lt;/section&gt;

    
&lt;/p&gt;

&lt;p&gt;You can even chain decoders and encoders together to get the equivalent to a traditional all-neurons-interconnected neural network. Basically, you get all the benefits of neural networks without having to save a whole darn weight matrix.&lt;/p&gt;

&lt;p&gt;


  
    &lt;figure &gt;
  



  &lt;label for=&#34;&#34; class=&#34;margin-toggle&#34;&gt;⊕&lt;/label&gt;
  &lt;input type=&#34;checkbox&#34; id=&#34;&#34; class=&#34;margin-toggle&#34;&gt;
  &lt;span class=&#34;marginnote&#34;&gt;
  

  
  
  
  
  

  &lt;/span&gt;


  
  &lt;img src=&#34;https://seanaubin.github.io/img/A-Way-Around-the-Coming-Performance-Walls-Neuromorphic-Hardware-with-Spiking-Neurons_3.png&#34; &gt;
  



&lt;/figure&gt;




  &lt;/section&gt;

    
&lt;/p&gt;

&lt;p&gt;


  
    &lt;figure &gt;
  



  &lt;label for=&#34;&#34; class=&#34;margin-toggle&#34;&gt;⊕&lt;/label&gt;
  &lt;input type=&#34;checkbox&#34; id=&#34;&#34; class=&#34;margin-toggle&#34;&gt;
  &lt;span class=&#34;marginnote&#34;&gt;
  

  
  
  
  
  

  &lt;/span&gt;


  
  &lt;img src=&#34;https://seanaubin.github.io/img/A-Way-Around-the-Coming-Performance-Walls-Neuromorphic-Hardware-with-Spiking-Neurons_4.png&#34; &gt;
  



&lt;/figure&gt;




  &lt;/section&gt;

    
&lt;/p&gt;

&lt;p&gt;With Nengo you can also do symbol-like and dynamic processing, as well as learning (both self-organising and error-driven) and combined all together you can build Spaun, the world’s largest functioning brain model. However, to explain all that stuff &lt;a href=&#34;http://www.amazon.com/How-Build-Brain-Architecture-Architectures/dp/0190262125&#34;&gt;would take a book&lt;/a&gt;. All you need to know, is that Nengo let’s you define models of human-like behaviour in spiking neurons. A spiking neuron computational core allows for distributed (lots of neurons), asynchronous and parallel (neurons don’t wait) computing.&lt;/p&gt;

&lt;p&gt;By the way, if you want non-GIF based, mathematical and methodical explanation of all this, check out &lt;a href=&#34;http://compneuro.uwaterloo.ca/research/syde-750.html&#34;&gt;the course notes to SYDE750&lt;/a&gt; which covers all of this. If you want more of an “explorable explanation” type of thing, check out the &lt;a href=&#34;https://github.com/nengo/nengo_gu&#34;&gt;Nengo GUI&lt;/a&gt; and work through the examples included.&lt;/p&gt;

&lt;h2 id=&#34;spiking-neuromorphic-hardware&#34;&gt;Spiking Neuromorphic Hardware&lt;/h2&gt;

&lt;p&gt;Brainstorm is a chip being built by Stanford and my lab right now that harnesses all the things talked about in the last section. It uses analogue spiking neurons which have a few exciting benefits that results in a much lower power consumption for computation.&lt;/p&gt;

&lt;p&gt;Firstly, a digital computer spends a lot of energy making sure it’s binary signals of “off” and “on” are never confused. As seen in the GIFs, analogue neurons distribute representation across neurons, so the accuracy doesn&amp;rsquo;t come from a single component, but from all the components working together.&lt;/p&gt;

&lt;p&gt;Secondly, the hardware is completely asynchronous, just like the brain, which means low power and fast computational speeds.&lt;/p&gt;

&lt;p&gt;To review, let’s contrast the development process of Nengo models and traditional software.&lt;/p&gt;

&lt;p&gt;


  
    &lt;figure &gt;
  



  &lt;label for=&#34;&#34; class=&#34;margin-toggle&#34;&gt;⊕&lt;/label&gt;
  &lt;input type=&#34;checkbox&#34; id=&#34;&#34; class=&#34;margin-toggle&#34;&gt;
  &lt;span class=&#34;marginnote&#34;&gt;
  

  
  
  
  
  

  &lt;/span&gt;


  
  &lt;img src=&#34;https://seanaubin.github.io/img/A-Way-Around-the-Coming-Performance-Walls-Neuromorphic-Hardware-with-Spiking-Neurons_5.png&#34; &gt;
  



&lt;/figure&gt;




  &lt;/section&gt;

    
&lt;/p&gt;

&lt;h2 id=&#34;problem-solved&#34;&gt;Problem Solved?&lt;/h2&gt;

&lt;p&gt;In the end, I don’t think the Nengo is the “right” approach to the future of neuromorphic hardware, it’s just one of the first ones. Just like it’s the first one to make a guess at what computations the brain can do. For now, I&amp;rsquo;m happy to ride the thrilling wave of being first and I’ll be ecstatic when something else surpasses us.&lt;/p&gt;

&lt;p&gt;I should also be clear that I don’t expect neuromorphic hardware to be the drop-in replacement of traditional computing hardware, at least not for a long time. I consider it more as an ambitious co-processor, to help out with things like control systems, computer vision and other tasks that humans excel at.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;If you liked this article and want to read more of my brain science posts, consider&lt;/em&gt; &lt;a href=&#34;https://uwaterloo.us15.list-manage.com/subscribe?u=d5612fe997cc72aac70c4ffe9&amp;amp;id=76226838bc&#34;&gt;&lt;em&gt;subscribing to my mailing list&lt;/em&gt;&lt;/a&gt;&lt;em&gt;.&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Understanding The Brain: Where Metaphors Limit You</title>
      <link>https://seanaubin.github.io/post/2015-09-17-Understanding-The-Brain-Where-Metaphors-Limit-You/</link>
      <pubDate>Thu, 17 Sep 2015 00:00:00 +0000</pubDate>
      
      <guid>https://seanaubin.github.io/post/2015-09-17-Understanding-The-Brain-Where-Metaphors-Limit-You/</guid>
      <description>

&lt;p&gt;There’s a lot of competition in cognitive science about which metaphor to use for the brain. This has been going on for centuries and keeping pace with technological advancement.&lt;/p&gt;

&lt;p&gt;In this article (which is basically a tl;dr of &lt;a href=&#34;http://www.arts.uwaterloo.ca/~celiasmi/Papers/eliasmith.moving%20beyond%20metaphors.jphil.pdf&#34;&gt;this paper from Chris Eliasmith&lt;/a&gt; and based off &lt;a href=&#34;http://slatestarcodex.com/2015/07/08/links-715-link-carbon-battery/#comment-218900&#34;&gt;this blog comment&lt;/a&gt;), I would like to justify that they’re all wrong. That all metaphors are wrong when trying to explain the brain and it’s better to just see the brain for what it is, based off the empirical observations we have from experiments. As evidence for this, I’ll talk about Spaun, the world’s largest functioning brain model that can solve IQ puzzles and transfer knowledge between tasks. The achievement of Spaun was made possible by the Neural Engineering Framework (NEF) and the Semantic Pointer Architecture (SPA) were used as a way to transcend brain-metaphors and to unite previous research done under existing ones.&lt;/p&gt;

&lt;h2 id=&#34;modern-brain-metaphors&#34;&gt;Modern Brain Metaphors&lt;/h2&gt;

&lt;p&gt;The three main brain metaphors in use today are Symbolicism (the brain thinks with symbols like a computer and neurons are pointless implementation details, see ACT-R), Dynamicism (the brain is a dynamic system that we should describe with differential equations like a Watt Governor, also neurons should still be ignored) and Connectionism (everyone should be paying attention to neurons. The brain is neurons and connection weights) .&lt;/p&gt;

&lt;h2 id=&#34;moving-beyond-metaphors&#34;&gt;Moving Beyond Metaphors&lt;/h2&gt;

&lt;p&gt;Using the NEF and SPA, we’re able to use components from all the previously mentioned paradigms to create a new paradigm in a similar way that waves and particles were combined to understand light in quantum physics. All the “computations” or “information transformations” occurring in our model is based on biologically plausible neurons, so we&amp;rsquo;ve got Connectionism covered. We’re also able to construct dynamical systems by feeding a neural network back into itself, so that’s Dynamicism. Finally, we’re able to represent vectors (multi-dimensional values) in neurons, which can be translated into symbols and manipulated, so we&amp;rsquo;ve covered Symbolism as well!&lt;/p&gt;

&lt;p&gt;This means we can take all the cool aspects from each of these systems and make cool things. Spaun uses symbols to solve the IQ puzzle I mentioned before, while dynamic systems is a more accurate description of it’s arm control and connectionist Deep Belief Networks (think Google stuff) form part of its vision system.&lt;/p&gt;

&lt;p&gt;However, you may have noticed I haven’t used any metaphors for NEF and SPA. That’s because there are none. Not to say its hard to explain. It isn’t. It’s just that it doesn’t fit into a specific metaphor. The NEF and SPA are just spiking neurons that represent vectors that can be manipulated via… Anyways, you end up just saying that the mind is a mind, the brain is a brain and a rose is a rose.&lt;/p&gt;

&lt;p&gt;I believe that it is this adhesion to trying to understand what computations the brain can do and then exploring it’s capabilities from there, that gives the NEF and SPA it’s power for unification and explanation. It’s also why I&amp;rsquo;ve decided to commit two years of my life to it.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>